{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Recurrent Neural Network Model\n",
    "1. Why sequence models\n",
    "- Notation\n",
    "- Recurrent Neural Network (RNN) Model\n",
    "- Backpropagation through time (BPTT)\n",
    "- Different types of RNNs\n",
    "- Language model and sequence generation\n",
    "- Vanishing gradients with RNNs\n",
    "- Gated Recurrent Unit (GRU)\n",
    "- Long Short term Memory (LSTM)\n",
    "- Bidirectional RNN\n",
    "- Deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Examples of sequential data\n",
    "![alt text](imgs\\0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Notation\n",
    "#### Input sequence\n",
    "- ![](imgs/001.png)\n",
    "Represents the t'th element (word) of the input sequence\n",
    "\n",
    "- ![](imgs/002.png) This represents the t'th element of the i'th input sequence (i.e. i is a training example/row)\n",
    "    \n",
    "- ![](imgs/003.png) This represents the lenght of the i'th training set example\n",
    "    \n",
    "#### Output sequence\n",
    "- ![](imgs/004.png) Same as above\n",
    "\n",
    "#### Dictionary\n",
    "- ![](imgs/005.png) A dictionary is a set of words used for one-hot encoding input elements. It's basically a list of n number of words, each with their own index.\n",
    "\n",
    "- This dictionary is used to one-hot encode input elements as shown below ![](imgs/006.png)\n",
    "\n",
    "- So x<sup>t</sup> is not the word itself but rather the vector representation of that word (representation could be made using one-hot encoding or some other way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Network Model\n",
    "#### Why not use a standard network?\n",
    "- We can use a standard network as given below, but it wouldn't work well (problems also given in picture): ![](imgs/008.png)\n",
    "- Using RNNs will also allow you to reduce the number of parameters since input sizes are huge\n",
    "#### Vizualising an RNN\n",
    "- Each network, besides getting the word vector as input, also gets the activation of the previous time step (i.e. the activation produced by the previous input vector) as input: ![](imgs/009.png)\n",
    "- a0 is a vector of 0s/randomly generated vector that's used as the input for an imaginary timestep 0. Also in the above example, Tx = Ty.\n",
    "- Another common representation of an RNN is as: ![](imgs/010.png), where the coloured box means that a timestep has passed.\n",
    "- RNNs go through the data from left to right and the parameters (weights) are shared (Wxa, Waa, Wya) (i.e. they're the same for all timesteps).\n",
    "- Weakness: only information from earlier inputs (of that sequence) can be used when producing y-hat for a given input. This is a problem in cases like: ![](imgs/011.png). Teddy represents a president in the first sentance but a toy in the next. It would be useful to get the president/bear word vector as input before getting \"Teddy\" (possible with Bidirectional RNNs - BRNNs - will be elaborated later).\n",
    "#### Forward Propagation\n",
    "- Equations: ![](imgs/012.png)\n",
    "- tanh is most commonly used as activation for hidden nodes. For output node, sigmoid is commonly used for binary classificaion tasks (like named-entity recognition), softmax for k-way classfication problem and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
